# Training Configuration for Yup250 Mamba Trading System

data:
  train_path: "data/features/1min"
  val_path: "data/features/1min" # Using same for now, usually we'd have separate folders or split in code
  symbols:
    - "EURUSD"
    - "XAUUSD"
  sequence_length: 480  # 8 hours @ 1 min (reduced for faster testing)
  bar_seconds: 60

model:
  d_input: 100  # 24 features * 4 TFs + 4 HMM = 100 features
  d_model: 256
  n_layers: 12
  d_state: 128
  n_assets: 2  # Updated to match available symbols
  dropout: 0.1
  use_multi_label: true

ensemble:
  enabled: false
  n_models: 3
  agreement_threshold: 0.67

training:
  batch_size: 64  # H100 80GB can handle large batches
  epochs: 50
  learning_rate: 0.0003  # Higher LR for larger batch
  weight_decay: 0.01
  gradient_clip: 1.0
  use_checkpoint: true
  checkpoint_every: 2  # Checkpoint every 2nd layer
  accumulation_steps: 1  # No accumulation needed with H100
  stride: 5  # Sample every 5th position (5x faster, still covers full data range)
  num_workers: 4  # Parallel data loading

augmentation:
  enabled: false
  methods:
    - time_warp
    - spread_noise
    - volatility_scaling
  probability: 0.5

multi_label:
  direction_weight: 1.0
  volatility_weight: 0.5
  magnitude_weight: 0.3
  confidence_weight: 0.2
  uncertainty_weight: 0.1

hmm:
  n_states: 3
  window_seconds: 3600

optimizer:
  name: "AdamW"
  betas: [0.9, 0.999]
  eps: 1.0e-8
  
scheduler:
  name: "CosineAnnealingLR"
  T_max: 50
  eta_min: 0.00001

loss:
  # Multi-task loss weights
  horizon_1min: 1.0
  horizon_5min: 0.8
  horizon_15min: 0.6
  horizon_1hour: 0.4

logging:
  log_dir: "logs/training"
  save_dir: "models/checkpoints"
  log_every: 10  # steps
  save_every: 1   # epochs
  tensorboard: true

validation:
  val_every: 1  # epochs
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 0.001
